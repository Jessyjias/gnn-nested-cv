{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Nested Cross Validation for GNN\n",
        "\n",
        "Notes on the dataset and model.\n",
        "\n",
        "- Data: subset of Lipophilicity data obtained from MoleculeNet (https://moleculenet.org/datasets-1).\n",
        "- Total of 840 datapoints are used.   \n",
        "- lipophilicity is a continuous number, hence the problem is framed as regression.   \n",
        "- A model from torch_geometric is used for experiment (AttentiveFP based on graph attention), Mean squared error (MSE) is used as loss function and evaluation.\n",
        "- Training parameters such as number of K-folds for outer and inner CV, number of trials for hyper-parameter tuning, number of training epochs are set to small values here for demo purposes.\n",
        "- Final training logs are saved in separate .log file. One example is uploaded under the same repo.\n",
        "- This notebook is used on Google Colab for training.\n",
        "\n",
        "About nested CV:\n",
        "- Outer CV is used for evaluation of model.\n",
        "- Inner CV is used for h-param tuning using train_data within a outer CV fold. The set of h-params is used to train model and evaluate on all inner-cv folds.\n",
        "- The best set of h-params (best mean score across inner CV) is used to train a model on the outer CV train set, and evaluated on the outer CV test set.\n"
      ],
      "metadata": {
        "id": "WeRA76LhI7c_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MzB-JxenNhUE"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import os\n",
        "from typing import List\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "import torch_geometric\n",
        "from torch_geometric.nn import AttentiveFP\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
        "from torch_geometric.data import Dataset\n",
        "from torch_geometric.utils import from_smiles\n",
        "\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from torch.nn import MSELoss\n",
        "from functools import partial\n",
        "\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCEgM8OQj0Xx",
        "outputId": "5e2f863d-a09d-4ea7-abe1-fd2e1771de7a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install torch_geometric\n",
        "# ! pip install rdkit\n",
        "# ! pip install sklearn\n",
        "# ! pip install optuna"
      ],
      "metadata": {
        "id": "0khxKZpVNm80"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom molecule dataset class\n",
        "\n",
        "- Class uses from_smiles function from torch.geometric to create graph data."
      ],
      "metadata": {
        "id": "gpVvgT2kj3oI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MoleculeDataset(Dataset):\n",
        "    \"\"\"Create a custom molecule dataset with filenames (list of filenames)\n",
        "\n",
        "    Attributes:\n",
        "      filenames: list of input dataset files (csv)\n",
        "      smiles_label: the column name of the smiles col\n",
        "      target_label: the target column name to be predicted (y label)\n",
        "      data_count: the total number of datapoints (not needed in initialization)\n",
        "    \"\"\"\n",
        "    def __init__(self, root, filenames:List[str], smiles_label:str, target_label:str, transform=None, pre_transform=None, pre_filter=None):\n",
        "        self.filenames = filenames\n",
        "        self.smiles_label = smiles_label\n",
        "        self.target_label = target_label\n",
        "        self.data_count = None\n",
        "        super(MoleculeDataset, self).__init__(root, transform, pre_transform, pre_filter)\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return self.filenames\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        if self.data_count is None:\n",
        "            self.data_count = self._calculate_num_data_objects()\n",
        "\n",
        "        return [f'data_{i}.pt' for i in range(self.data_count)]\n",
        "\n",
        "    def _calculate_num_data_objects(self): #calculate once when initializing\n",
        "        count = 0\n",
        "        for raw_path in self.raw_paths:\n",
        "            df = pd.read_csv(raw_path)\n",
        "            count += len(df)\n",
        "        return count\n",
        "\n",
        "    def download(self):\n",
        "        pass\n",
        "\n",
        "    def process(self):\n",
        "        idx = 0\n",
        "        for raw_path in self.raw_paths:\n",
        "            df = pd.read_csv(self.raw_paths[0]).reset_index()\n",
        "            for i, smile in enumerate(df[self.smiles_label]):\n",
        "              g = from_smiles(smile)\n",
        "              g.x = g.x.float()\n",
        "              y = torch.tensor(df[self.target_label][i], dtype=torch.float).view(1, -1)\n",
        "              g.y = y\n",
        "\n",
        "              torch.save(g, os.path.join(self.processed_dir, f'data_{idx}.pt'))\n",
        "              idx += 1\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.processed_file_names)\n",
        "\n",
        "    def get(self, idx):\n",
        "        data = torch.load(os.path.join(self.processed_dir, f'data_{idx}.pt'))\n",
        "        return data"
      ],
      "metadata": {
        "id": "-fGlIhbLN7KE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train, validate and test functions"
      ],
      "metadata": {
        "id": "XQTFI4kvj8ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def train_one_epoch(model:torch_geometric.nn, train_loader:DataLoader, optimizer:torch.optim, loss_fn:torch.nn) -> float:\n",
        "    \"\"\"\n",
        "    Train a torch geometric model for 1 epoch.\n",
        "    Args:\n",
        "      model: torch_geometric.nn model to be trained\n",
        "      train_loader: DataLoader for train data\n",
        "      optimizer: torch optimizer\n",
        "      loss_fn: loss function from torch.nn\n",
        "\n",
        "    Returns:\n",
        "      loss_ep: loss for current epoch (float)\n",
        "    \"\"\"\n",
        "    running_loss, num_data = 0,0\n",
        "    for _, data in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        # reset grad\n",
        "        optimizer.zero_grad()\n",
        "        ## get pred from model\n",
        "        pred = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        # get loss and grads\n",
        "        loss = loss_fn(pred, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        ## mse need to take average across number of samples.\n",
        "        ## num of data in last batch may not be same as batch size.\n",
        "        running_loss += float(loss) * data.num_graphs ##expand loss on batch by batch size\n",
        "        num_data += data.num_graphs\n",
        "    loss_ep = running_loss/num_data\n",
        "    return loss_ep\n",
        "\n",
        "def train(num_epochs:int, model:torch_geometric.nn, train_loader:DataLoader, optimizer:torch.optim, loss_fn:torch.nn):\n",
        "    \"\"\"\n",
        "    Train a torch geometric model for specified epoch numbers\n",
        "    Args:\n",
        "      num_epochs: num epochs\n",
        "      model: torch_geometric.nn model to be trained\n",
        "      train_loader: DataLoader for train data\n",
        "      optimizer: torch optimizer\n",
        "      loss_fn: loss function from torch.nn\n",
        "\n",
        "    Returns:\n",
        "      None\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    for ep in range(num_epochs):\n",
        "        loss = train_one_epoch(model, train_loader, optimizer, loss_fn)\n",
        "        logger.info(f\"Epoch {ep} | Train Loss (MSE) {loss}\")\n",
        "        ## can add mlflow metrics logging / early stopping here\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model:torch_geometric.nn, loader:DataLoader, loss_fn:torch.nn) -> float:\n",
        "    \"\"\"\n",
        "    Compute validation loss for model\n",
        "    Args:\n",
        "      model: torch_geometric.nn model to be validated\n",
        "      loader: DataLoader for validation data å\n",
        "      loss_fn:l oss function from torch.nn\n",
        "\n",
        "    Returns:\n",
        "      valid_loss: validation loss (float)\n",
        "    \"\"\"\n",
        "    valid_loss = 0\n",
        "    num_data = 0\n",
        "    model.eval()\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        pred = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        loss = loss_fn(pred, data.y)\n",
        "        valid_loss += float(loss) * data.num_graphs ##expand loss on batch by batch size\n",
        "        num_data += data.num_graphs\n",
        "    valid_loss = valid_loss/num_data\n",
        "    return valid_loss\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model:torch_geometric.nn, loader:DataLoader, loss_fn:torch.nn) -> float:\n",
        "    \"\"\"\n",
        "    Test/Evaluate the model\n",
        "    Args:\n",
        "      model: torch_geometric.nn model to be validated\n",
        "      loader: DataLoader for validation data\n",
        "      loss_fn: loss function from torch.nn\n",
        "\n",
        "    Returns:\n",
        "      mse: returns mse (float)\n",
        "    \"\"\"\n",
        "    output = []\n",
        "    model.eval()\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        pred = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        loss = loss_fn(pred, data.y)\n",
        "        concatenated_batch = torch.cat((pred, data.y), dim=1) ## dim = [num_test, 2]\n",
        "        output.append(concatenated_batch)\n",
        "\n",
        "    # Stack the tensors along batch of data dimension\n",
        "    stacked_output = torch.cat(output, dim=0) #concat by rows - all data in batch\n",
        "\n",
        "    ## can add additional evaluation metrics and/or return actual output for plotting\n",
        "    mse = mean_squared_error(stacked_output[1], stacked_output[0])\n",
        "    logger.info(f\"Test Evaluation MSE: {mse}\")\n",
        "\n",
        "    return mse\n",
        ""
      ],
      "metadata": {
        "id": "Fq4AzchSn_bX"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inner CV for h-param optimization"
      ],
      "metadata": {
        "id": "sGeR-yLX_7X_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cv_objective(trial:int, k_inner:int, model:torch_geometric.nn, train_val_data:Dataset, loss_fn:torch.nn, num_epochs:int):\n",
        "    \"\"\"\n",
        "    Inner CV to perform h-param optimization.\n",
        "    Args:\n",
        "      trial: trial number\n",
        "      k_inner: number of inner CV folds\n",
        "      model: torch_geometric.nn model to be trained\n",
        "      train_val_data: Dataset containing train and validation data for inner CV\n",
        "      loss_fn: loss function from torch.nn\n",
        "\n",
        "    Returns: mean of inner cv loss\n",
        "\n",
        "    \"\"\"\n",
        "    logger.info(f'Performing inner CV hparam optimization')\n",
        "    fold = KFold(n_splits=k_inner, shuffle=True, random_state=0)\n",
        "    inner_cv_loss = []\n",
        "    # get hparams for this trial\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=False)\n",
        "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=False)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr,\n",
        "                                weight_decay=weight_decay)\n",
        "    for fold_idx, (train_idx, valid_idx) in enumerate(fold.split(range(len(train_val_data)))):\n",
        "        train_data = torch.utils.data.Subset(dataset, train_idx)\n",
        "        valid_data = torch.utils.data.Subset(dataset, valid_idx)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_data,\n",
        "            batch_size=32,\n",
        "            shuffle=True\n",
        "        )\n",
        "        valid_loader = DataLoader(\n",
        "            valid_data,\n",
        "            batch_size=32,\n",
        "            shuffle=True,\n",
        "        )\n",
        "        ## reset model\n",
        "        model.reset_parameters()\n",
        "        # train model\n",
        "        train(num_epochs, model, train_loader, optimizer, loss_fn)\n",
        "        # get valid loss\n",
        "        loss_v = validate(model, valid_loader, loss_fn)\n",
        "        inner_cv_loss.append(loss_v)\n",
        "    return np.mean(inner_cv_loss)\n"
      ],
      "metadata": {
        "id": "KsNC8AI2kelU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Outer CV"
      ],
      "metadata": {
        "id": "8tZZFLSbqkK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "def nested_cv_experiment(config:dict):\n",
        "    \"\"\"\n",
        "    Run the nested CV experiment using config dictionary\n",
        "    Args:\n",
        "      config: config for the experiment, including model, dataset, loss, optimizer, k-folds settings and epochs.\n",
        "    \"\"\"\n",
        "    ## training experiment settings unpack\n",
        "    model = config['model']\n",
        "    dataset = config['dataset']\n",
        "    loss_fn = config['loss_fn']\n",
        "    optimizer_class = config['optimizer_class']\n",
        "    num_epochs = config['num_epochs']\n",
        "    batch_size = config['batch_size']\n",
        "    k_inner = config['k_inner']\n",
        "    k_outer = config['k_outer']\n",
        "    objective = config['objective']\n",
        "    n_trials = config['n_trials']\n",
        "    timeout_optuna = config['timeout_optuna']\n",
        "\n",
        "    cv_outer = KFold(n_splits=k_outer, shuffle=True, random_state=22)\n",
        "    eval_loss = []\n",
        "    ## Outer CV\n",
        "    for fold, (train_ids, test_ids) in enumerate(cv_outer.split(range(len(dataset)))):\n",
        "        logger.info(f\"{fold}-FOLD out of {k_outer}\")\n",
        "        logger.info(f\"Train length: {len(train_ids)} Validate length: {len(test_ids)} SUM:, {len(test_ids)+len(train_ids)}\")\n",
        "        train_data = torch.utils.data.Subset(dataset, train_ids)\n",
        "        test_data = torch.utils.data.Subset(dataset, test_ids)\n",
        "\n",
        "        ## start inner CV for finding the best hparams on this fold\n",
        "        model.to(device).reset_parameters()\n",
        "\n",
        "        logger.info('Optimizing hparams')\n",
        "        study = optuna.create_study(direction=\"minimize\")\n",
        "        cv_objective = partial(objective, k_inner=k_inner, model=model, train_val_data=train_data, loss_fn=loss_fn, num_epochs=num_epochs) # perform inner CV for h-param tuning using train data from outer CV split\n",
        "        study.optimize(cv_objective, n_trials=2, timeout=timeout_optuna)\n",
        "\n",
        "        best_params = study.best_trial.params\n",
        "        logger.info(\"Best h-params: \", best_params)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=best_params['lr'],\n",
        "                                    weight_decay=best_params['weight_decay'])\n",
        "\n",
        "        ## retrain model on best params optimizer\n",
        "        logger.info('Test on unseen data using the model+best hparams from inner loop')\n",
        "        model.reset_parameters()\n",
        "        train_loader = DataLoader(train_data, batch_size=32)\n",
        "        train(num_epochs, model, train_loader, optimizer, loss_fn)\n",
        "\n",
        "        test_loader = DataLoader(test_data, batch_size=32)\n",
        "        mse_fold = test(model, test_loader, loss_fn)\n",
        "\n",
        "        eval_loss.append(mse_fold)\n",
        "\n",
        "    logger.info(f'Mean MSE across Outer CV: {np.mean(eval_loss)}')"
      ],
      "metadata": {
        "id": "PPaMY8eyd7l8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run experiment"
      ],
      "metadata": {
        "id": "wV-z4h_b__US"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Set path to data and working directory for dataset creation.\n",
        "root_path = '/content/drive/MyDrive/Colab Notebooks/data_gnn/data/'\n",
        "working_path = '/content/drive/MyDrive/Colab Notebooks/data_gnn/working'\n",
        "data_files = []\n",
        "for filename in os.listdir(root_path):\n",
        "    data_files.append(root_path+filename)\n",
        "\n",
        "print(data_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmrJd23xbEiT",
        "outputId": "0780c51e-54b3-4243-a7ab-5f14baf74a37"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/Colab Notebooks/data_gnn/data/Lipophilicity.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking dataset  "
      ],
      "metadata": {
        "id": "lhzIELiQAGoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MoleculeDataset(root=working_path, filenames=data_files, smiles_label='smiles', target_label='exp')\n",
        "\n",
        "dataset[0], dataset[2]\n",
        "\n",
        "## Input data feature: in_dim=9, edge_dim=3"
      ],
      "metadata": {
        "id": "QLytgkWIaZX6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c043f336-5c3d-4eed-9752-1ba8b10cba34"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Data(x=[22, 9], edge_index=[2, 48], edge_attr=[48, 3], smiles='CC(C)c1ccc2Oc3nc(N)c(cc3C(=O)c2c1)C(=O)O', y=[1, 1]),\n",
              " Data(x=[28, 9], edge_index=[2, 60], edge_attr=[60, 3], smiles='CN(C)C(=O)N[C@@H]1CC[C@@H](CCN2CCN(CC2)c3ccc(Cl)c(Cl)c3)CC1', y=[1, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "# set experiment config\n",
        "config = {\n",
        "    'dataset': MoleculeDataset(root=working_path, filenames=data_files, smiles_label='smiles', target_label='exp'),\n",
        "    'model': AttentiveFP(in_channels=9, hidden_channels=64, out_channels=1,\n",
        "                    edge_dim=3, num_layers=2, num_timesteps=2,\n",
        "                    dropout=0.2),\n",
        "    'loss_fn': MSELoss(),\n",
        "    'optimizer_class': torch.optim.Adam,\n",
        "    'scheduler_params': {'gamma': 0.9},\n",
        "    'num_epochs': 2,\n",
        "    'batch_size': 32,\n",
        "    'k_inner': 3,\n",
        "    'k_outer': 5,\n",
        "    'objective': cv_objective,\n",
        "    'n_trials': 2,\n",
        "    'timeout_optuna': 300\n",
        "}\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# get logger\n",
        "log_file_path = '/content/drive/MyDrive/Colab Notebooks/data_gnn/experiment_attentiveFP_nestedcv.log'\n",
        "logging.basicConfig(filename=log_file_path,\n",
        "                    format='%(asctime)s %(message)s',\n",
        "                    level=logging.DEBUG,\n",
        "                    filemode='w',  force=True)\n",
        "logger = logging.getLogger('log')\n",
        "\n",
        "# run experiment on current config\n",
        "nested_cv_experiment(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJVQv54d_VfH",
        "outputId": "6497e67f-cca3-4366-de73-0b71cb7f38c9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-07-24 05:04:49,000] A new study created in memory with name: no-name-efee1c96-a188-4f0e-9183-7d2fa708a564\n",
            "[I 2024-07-24 05:05:15,377] Trial 0 finished with value: 1.6451580779893058 and parameters: {'lr': 0.008658744473535875, 'weight_decay': 0.00025948971735842744}. Best is trial 0 with value: 1.6451580779893058.\n",
            "[I 2024-07-24 05:05:34,627] Trial 1 finished with value: 1.5202242732048035 and parameters: {'lr': 0.009545491245148186, 'weight_decay': 8.527351248477438e-05}. Best is trial 1 with value: 1.5202242732048035.\n",
            "[I 2024-07-24 05:05:42,987] A new study created in memory with name: no-name-082f36cb-ef23-42a8-a9c5-5ead514b0f9e\n",
            "[I 2024-07-24 05:06:02,015] Trial 0 finished with value: 1.8114200433095295 and parameters: {'lr': 0.006964723383840363, 'weight_decay': 0.0006996485545470178}. Best is trial 0 with value: 1.8114200433095295.\n",
            "[I 2024-07-24 05:06:21,210] Trial 1 finished with value: 1.492532233397166 and parameters: {'lr': 0.002992464149670449, 'weight_decay': 1.7609854138450537e-05}. Best is trial 1 with value: 1.492532233397166.\n",
            "[I 2024-07-24 05:06:30,512] A new study created in memory with name: no-name-139d093c-d1df-44ec-8a09-30c3b1fcce96\n",
            "[I 2024-07-24 05:06:51,140] Trial 0 finished with value: 1.503690305210295 and parameters: {'lr': 0.0021996273493665, 'weight_decay': 0.0009020793929811481}. Best is trial 0 with value: 1.503690305210295.\n",
            "[I 2024-07-24 05:07:11,309] Trial 1 finished with value: 1.3920365628742035 and parameters: {'lr': 0.0018038688432514102, 'weight_decay': 0.0009715144339286272}. Best is trial 1 with value: 1.3920365628742035.\n",
            "[I 2024-07-24 05:07:20,135] A new study created in memory with name: no-name-feb626ed-6d29-463c-bf66-2bd01f097900\n",
            "[I 2024-07-24 05:07:40,436] Trial 0 finished with value: 1.4326474240847997 and parameters: {'lr': 0.008436681128962663, 'weight_decay': 0.00031051524654821905}. Best is trial 0 with value: 1.4326474240847997.\n",
            "[I 2024-07-24 05:08:02,664] Trial 1 finished with value: 1.652911501271384 and parameters: {'lr': 0.0021224651581005565, 'weight_decay': 0.000988017567013687}. Best is trial 0 with value: 1.4326474240847997.\n",
            "[I 2024-07-24 05:08:16,191] A new study created in memory with name: no-name-0c0e51bf-06a7-4dcb-aadb-b03b9c91515d\n",
            "[I 2024-07-24 05:08:41,391] Trial 0 finished with value: 1.652192127136957 and parameters: {'lr': 0.006253867832891462, 'weight_decay': 0.0001680664583725346}. Best is trial 0 with value: 1.652192127136957.\n",
            "[I 2024-07-24 05:09:04,223] Trial 1 finished with value: 1.5192730937685284 and parameters: {'lr': 0.008866367639041537, 'weight_decay': 0.0003551810888249911}. Best is trial 1 with value: 1.5192730937685284.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check experiment log\n",
        "print(Path(log_file_path).read_text())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI7y8ItYg1XD",
        "outputId": "6925ecbd-06c6-4a1c-f91b-cff38d9b1ba8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-24 05:04:48,996 0-FOLD out of 5\n",
            "2024-07-24 05:04:48,997 Train length: 672 Validate length: 168 SUM:, 840\n",
            "2024-07-24 05:04:48,999 Optimizing hparams\n",
            "2024-07-24 05:04:49,002 Performing inner CV hparam optimization\n",
            "2024-07-24 05:04:52,587 Epoch 0 | Train Loss (MSE) 26.069338176931655\n",
            "2024-07-24 05:04:56,787 Epoch 1 | Train Loss (MSE) 1.9465930802481515\n",
            "2024-07-24 05:05:03,978 Epoch 0 | Train Loss (MSE) 16.121856825692312\n",
            "2024-07-24 05:05:07,680 Epoch 1 | Train Loss (MSE) 3.172085762023926\n",
            "2024-07-24 05:05:11,671 Epoch 0 | Train Loss (MSE) 9.440867551735469\n",
            "2024-07-24 05:05:14,148 Epoch 1 | Train Loss (MSE) 2.098649493285588\n",
            "2024-07-24 05:05:15,379 Performing inner CV hparam optimization\n",
            "2024-07-24 05:05:18,253 Epoch 0 | Train Loss (MSE) 49.89956702504839\n",
            "2024-07-24 05:05:20,762 Epoch 1 | Train Loss (MSE) 2.0127105627741133\n",
            "2024-07-24 05:05:24,295 Epoch 0 | Train Loss (MSE) 5.439251380307334\n",
            "2024-07-24 05:05:26,688 Epoch 1 | Train Loss (MSE) 1.9557387573378426\n",
            "2024-07-24 05:05:30,743 Epoch 0 | Train Loss (MSE) 3.754374861717224\n",
            "2024-07-24 05:05:33,474 Epoch 1 | Train Loss (MSE) 1.4775968279157365\n",
            "2024-07-24 05:05:34,628 Best h-params: \n",
            "2024-07-24 05:05:34,632 Test on unseen data using the model+best hparams from inner loop\n",
            "2024-07-24 05:05:38,268 Epoch 0 | Train Loss (MSE) 10.951640912464686\n",
            "2024-07-24 05:05:42,108 Epoch 1 | Train Loss (MSE) 1.7000090678532918\n",
            "2024-07-24 05:05:42,983 Test Evaluation MSE: 0.7205361127853394\n",
            "2024-07-24 05:05:42,983 1-FOLD out of 5\n",
            "2024-07-24 05:05:42,983 Train length: 672 Validate length: 168 SUM:, 840\n",
            "2024-07-24 05:05:42,987 Optimizing hparams\n",
            "2024-07-24 05:05:42,990 Performing inner CV hparam optimization\n",
            "2024-07-24 05:05:45,839 Epoch 0 | Train Loss (MSE) 27.11099338531494\n",
            "2024-07-24 05:05:48,227 Epoch 1 | Train Loss (MSE) 2.5510313510894775\n",
            "2024-07-24 05:05:51,786 Epoch 0 | Train Loss (MSE) 5.403518157345908\n",
            "2024-07-24 05:05:54,179 Epoch 1 | Train Loss (MSE) 1.7704129815101624\n",
            "2024-07-24 05:05:58,077 Epoch 0 | Train Loss (MSE) 9.57170832157135\n",
            "2024-07-24 05:06:00,904 Epoch 1 | Train Loss (MSE) 1.7993489163262504\n",
            "2024-07-24 05:06:02,017 Performing inner CV hparam optimization\n",
            "2024-07-24 05:06:04,533 Epoch 0 | Train Loss (MSE) 2.951813953263419\n",
            "2024-07-24 05:06:06,971 Epoch 1 | Train Loss (MSE) 1.4920063955443246\n",
            "2024-07-24 05:06:10,643 Epoch 0 | Train Loss (MSE) 4.464568606444767\n",
            "2024-07-24 05:06:13,623 Epoch 1 | Train Loss (MSE) 1.7898253543036324\n",
            "2024-07-24 05:06:17,428 Epoch 0 | Train Loss (MSE) 3.5895890252930776\n",
            "2024-07-24 05:06:20,072 Epoch 1 | Train Loss (MSE) 1.5359051142420088\n",
            "2024-07-24 05:06:21,211 Best h-params: \n",
            "2024-07-24 05:06:21,214 Test on unseen data using the model+best hparams from inner loop\n",
            "2024-07-24 05:06:25,349 Epoch 0 | Train Loss (MSE) 7.374721402213687\n",
            "2024-07-24 05:06:29,659 Epoch 1 | Train Loss (MSE) 1.861585565975734\n",
            "2024-07-24 05:06:30,508 Test Evaluation MSE: 0.24946333467960358\n",
            "2024-07-24 05:06:30,509 2-FOLD out of 5\n",
            "2024-07-24 05:06:30,509 Train length: 672 Validate length: 168 SUM:, 840\n",
            "2024-07-24 05:06:30,511 Optimizing hparams\n",
            "2024-07-24 05:06:30,513 Performing inner CV hparam optimization\n",
            "2024-07-24 05:06:33,336 Epoch 0 | Train Loss (MSE) 7.308099814823696\n",
            "2024-07-24 05:06:35,946 Epoch 1 | Train Loss (MSE) 2.144741177558899\n",
            "2024-07-24 05:06:39,987 Epoch 0 | Train Loss (MSE) 4.062236053603036\n",
            "2024-07-24 05:06:42,852 Epoch 1 | Train Loss (MSE) 1.6944026776722498\n",
            "2024-07-24 05:06:46,869 Epoch 0 | Train Loss (MSE) 4.1503690992082864\n",
            "2024-07-24 05:06:50,001 Epoch 1 | Train Loss (MSE) 1.8880003264972143\n",
            "2024-07-24 05:06:51,141 Performing inner CV hparam optimization\n",
            "2024-07-24 05:06:53,968 Epoch 0 | Train Loss (MSE) 3.5513758659362793\n",
            "2024-07-24 05:06:56,884 Epoch 1 | Train Loss (MSE) 1.646525434085301\n",
            "2024-07-24 05:07:00,613 Epoch 0 | Train Loss (MSE) 7.416520365646908\n",
            "2024-07-24 05:07:03,217 Epoch 1 | Train Loss (MSE) 1.8862335341317313\n",
            "2024-07-24 05:07:07,164 Epoch 0 | Train Loss (MSE) 8.762610392911094\n",
            "2024-07-24 05:07:10,167 Epoch 1 | Train Loss (MSE) 2.0433094586644853\n",
            "2024-07-24 05:07:11,311 Best h-params: \n",
            "2024-07-24 05:07:11,314 Test on unseen data using the model+best hparams from inner loop\n",
            "2024-07-24 05:07:15,216 Epoch 0 | Train Loss (MSE) 5.143171804291861\n",
            "2024-07-24 05:07:19,216 Epoch 1 | Train Loss (MSE) 1.5861867723010836\n",
            "2024-07-24 05:07:20,130 Test Evaluation MSE: 0.10957490652799606\n",
            "2024-07-24 05:07:20,130 3-FOLD out of 5\n",
            "2024-07-24 05:07:20,131 Train length: 672 Validate length: 168 SUM:, 840\n",
            "2024-07-24 05:07:20,134 Optimizing hparams\n",
            "2024-07-24 05:07:20,139 Performing inner CV hparam optimization\n",
            "2024-07-24 05:07:23,176 Epoch 0 | Train Loss (MSE) 15.670487208025795\n",
            "2024-07-24 05:07:26,023 Epoch 1 | Train Loss (MSE) 1.7268817680222648\n",
            "2024-07-24 05:07:29,817 Epoch 0 | Train Loss (MSE) 22.667606864656722\n",
            "2024-07-24 05:07:32,321 Epoch 1 | Train Loss (MSE) 3.3573025294712613\n",
            "2024-07-24 05:07:36,343 Epoch 0 | Train Loss (MSE) 10.893845098359245\n",
            "2024-07-24 05:07:39,246 Epoch 1 | Train Loss (MSE) 2.7827809367861067\n",
            "2024-07-24 05:07:40,437 Performing inner CV hparam optimization\n",
            "2024-07-24 05:07:43,100 Epoch 0 | Train Loss (MSE) 3.53766291482108\n",
            "2024-07-24 05:07:45,958 Epoch 1 | Train Loss (MSE) 1.5666478276252747\n",
            "2024-07-24 05:07:50,407 Epoch 0 | Train Loss (MSE) 3.1124511105673656\n",
            "2024-07-24 05:07:53,517 Epoch 1 | Train Loss (MSE) 1.7146334477833338\n",
            "2024-07-24 05:07:58,216 Epoch 0 | Train Loss (MSE) 4.812606223991939\n",
            "2024-07-24 05:08:01,281 Epoch 1 | Train Loss (MSE) 1.820297999041421\n",
            "2024-07-24 05:08:02,666 Best h-params: \n",
            "2024-07-24 05:08:02,672 Test on unseen data using the model+best hparams from inner loop\n",
            "2024-07-24 05:08:09,397 Epoch 0 | Train Loss (MSE) 11.624237821215676\n",
            "2024-07-24 05:08:14,736 Epoch 1 | Train Loss (MSE) 1.5671267339161463\n",
            "2024-07-24 05:08:16,186 Test Evaluation MSE: 0.0019980089273303747\n",
            "2024-07-24 05:08:16,187 4-FOLD out of 5\n",
            "2024-07-24 05:08:16,187 Train length: 672 Validate length: 168 SUM:, 840\n",
            "2024-07-24 05:08:16,191 Optimizing hparams\n",
            "2024-07-24 05:08:16,193 Performing inner CV hparam optimization\n",
            "2024-07-24 05:08:19,835 Epoch 0 | Train Loss (MSE) 29.132176842008317\n",
            "2024-07-24 05:08:22,934 Epoch 1 | Train Loss (MSE) 4.173726899283273\n",
            "2024-07-24 05:08:27,955 Epoch 0 | Train Loss (MSE) 13.140173452241081\n",
            "2024-07-24 05:08:31,276 Epoch 1 | Train Loss (MSE) 1.8539689012936182\n",
            "2024-07-24 05:08:36,566 Epoch 0 | Train Loss (MSE) 6.52350868497576\n",
            "2024-07-24 05:08:40,133 Epoch 1 | Train Loss (MSE) 1.6348934258733476\n",
            "2024-07-24 05:08:41,393 Performing inner CV hparam optimization\n",
            "2024-07-24 05:08:44,270 Epoch 0 | Train Loss (MSE) 7.917773468153817\n",
            "2024-07-24 05:08:47,235 Epoch 1 | Train Loss (MSE) 1.705918320587703\n",
            "2024-07-24 05:08:51,877 Epoch 0 | Train Loss (MSE) 10.25947346006121\n",
            "2024-07-24 05:08:55,205 Epoch 1 | Train Loss (MSE) 2.7283865894590105\n",
            "2024-07-24 05:08:58,994 Epoch 0 | Train Loss (MSE) 9.62975549697876\n",
            "2024-07-24 05:09:02,124 Epoch 1 | Train Loss (MSE) 1.7938830852508545\n",
            "2024-07-24 05:09:04,225 Best h-params: \n",
            "2024-07-24 05:09:04,228 Test on unseen data using the model+best hparams from inner loop\n",
            "2024-07-24 05:09:08,684 Epoch 0 | Train Loss (MSE) 10.56649945463453\n",
            "2024-07-24 05:09:13,018 Epoch 1 | Train Loss (MSE) 1.635736414364406\n",
            "2024-07-24 05:09:14,017 Test Evaluation MSE: 1.2784576416015625\n",
            "2024-07-24 05:09:14,020 Mean MSE across Outer CV: 0.47200602293014526\n",
            "\n"
          ]
        }
      ]
    }
  ]
}