{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Nested Cross Validation for GNN\n",
        "\n",
        "Notes on the dataset and model.\n",
        "\n",
        "- Data: subset of Lipophilicity data obtained from MoleculeNet (https://moleculenet.org/datasets-1).\n",
        "- Total of 840 datapoints are used.   \n",
        "- lipophilicity is a continuous number, hence the problem is framed as regression.   \n",
        "- A model from torch_geometric is used for experiment (AttentiveFP based on graph attention), Mean squared error (MSE) is used as loss function and evaluation.\n",
        "- Training parameters such as number of K-folds for outer and inner CV, number of trials for hyper-parameter tuning, number of training epochs are set to small values here for demo purposes.\n",
        "- Optuna package is used for h-param tuning.\n",
        "- Final training logs are saved in separate .log file. One example is uploaded under the same repo.\n",
        "- This notebook is used on Google Colab for training.\n",
        "\n",
        "About nested CV:\n",
        "- Outer CV is used for evaluation of model.\n",
        "- Inner CV is used for h-param tuning using train_data within a outer CV fold. The set of h-params is used to train model and evaluate on all inner-cv folds.\n",
        "- The best set of h-params (best mean score across inner CV) is used to train a model on the outer CV train set, and evaluated on the outer CV test set.\n"
      ],
      "metadata": {
        "id": "WeRA76LhI7c_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzB-JxenNhUE"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import os\n",
        "from typing import List\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import SubsetRandomSampler\n",
        "import torch_geometric\n",
        "from torch_geometric.nn import AttentiveFP\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.data import Dataset\n",
        "from torch_geometric.utils import from_smiles\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from torch.nn import MSELoss\n",
        "from functools import partial\n",
        "\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCEgM8OQj0Xx",
        "outputId": "321033d2-7ccc-4a54-f696-fc205a9039e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torch_geometric\n",
        "! pip install rdkit\n",
        "! pip install sklearn\n",
        "! pip install optuna"
      ],
      "metadata": {
        "id": "0khxKZpVNm80"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom molecule dataset class\n",
        "\n",
        "- Class uses from_smiles function from torch.geometric to create graph data."
      ],
      "metadata": {
        "id": "gpVvgT2kj3oI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MoleculeDataset(Dataset):\n",
        "    \"\"\"Create a custom molecule dataset with filenames (list of filenames)\n",
        "\n",
        "    Attributes:\n",
        "      filenames: list of input dataset files (csv)\n",
        "      smiles_label: the column name of the smiles col\n",
        "      target_label: the target column name to be predicted (y label)\n",
        "      data_count: the total number of datapoints (not needed in initialization)\n",
        "    \"\"\"\n",
        "    def __init__(self, root, filenames:List[str], smiles_label:str, target_label:str, transform=None, pre_transform=None, pre_filter=None):\n",
        "        self.filenames = filenames\n",
        "        self.smiles_label = smiles_label\n",
        "        self.target_label = target_label\n",
        "        self.data_count = None\n",
        "        super(MoleculeDataset, self).__init__(root, transform, pre_transform, pre_filter)\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return self.filenames\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        if self.data_count is None:\n",
        "            self.data_count = self._calculate_num_data_objects()\n",
        "\n",
        "        return [f'data_{i}.pt' for i in range(self.data_count)]\n",
        "\n",
        "    def _calculate_num_data_objects(self): #calculate once when initializing\n",
        "        count = 0\n",
        "        for raw_path in self.raw_paths:\n",
        "            df = pd.read_csv(raw_path)\n",
        "            count += len(df)\n",
        "        return count\n",
        "\n",
        "    def download(self):\n",
        "        pass\n",
        "\n",
        "    def process(self):\n",
        "        idx = 0\n",
        "        for raw_path in self.raw_paths:\n",
        "            df = pd.read_csv(self.raw_paths[0]).reset_index()\n",
        "            for i, smile in enumerate(df[self.smiles_label]):\n",
        "              g = from_smiles(smile)\n",
        "              g.x = g.x.float()\n",
        "              y = torch.tensor(df[self.target_label][i], dtype=torch.float).view(1, -1)\n",
        "              g.y = y\n",
        "\n",
        "              torch.save(g, os.path.join(self.processed_dir, f'data_{idx}.pt'))\n",
        "              idx += 1\n",
        "\n",
        "    def len(self):\n",
        "        return len(self.processed_file_names)\n",
        "\n",
        "    def get(self, idx):\n",
        "        data = torch.load(os.path.join(self.processed_dir, f'data_{idx}.pt'))\n",
        "        return data"
      ],
      "metadata": {
        "id": "-fGlIhbLN7KE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train, validate and test functions"
      ],
      "metadata": {
        "id": "XQTFI4kvj8ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model:torch_geometric.nn, train_loader:DataLoader, optimizer:torch.optim, loss_fn:torch.nn) -> float:\n",
        "    \"\"\"\n",
        "    Train a torch geometric model for 1 epoch.\n",
        "    Args:\n",
        "      model: torch_geometric.nn model to be trained\n",
        "      train_loader: DataLoader for train data\n",
        "      optimizer: torch optimizer\n",
        "      loss_fn: loss function from torch.nn\n",
        "\n",
        "    Returns:\n",
        "      loss_ep: loss for current epoch (float)\n",
        "    \"\"\"\n",
        "    running_loss, num_data = 0,0\n",
        "    for _, data in enumerate(train_loader):\n",
        "        data = data.to(device)\n",
        "        # reset grad\n",
        "        optimizer.zero_grad()\n",
        "        ## get pred from model\n",
        "        pred = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        # get loss and grads\n",
        "        loss = loss_fn(pred, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        ## mse need to take average across number of samples.\n",
        "        ## num of data in last batch may not be same as batch size.\n",
        "        running_loss += float(loss) * data.num_graphs ##expand loss on batch by batch size\n",
        "        num_data += data.num_graphs\n",
        "    loss_ep = running_loss/num_data\n",
        "    return loss_ep\n",
        "\n",
        "def train(num_epochs:int, model:torch_geometric.nn, train_loader:DataLoader, optimizer:torch.optim, loss_fn:torch.nn):\n",
        "    \"\"\"\n",
        "    Train a torch geometric model for specified epoch numbers\n",
        "    Args:\n",
        "      num_epochs: num epochs\n",
        "      model: torch_geometric.nn model to be trained\n",
        "      train_loader: DataLoader for train data\n",
        "      optimizer: torch optimizer\n",
        "      loss_fn: loss function from torch.nn\n",
        "\n",
        "    Returns:\n",
        "      None\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    for ep in range(num_epochs):\n",
        "        loss = train_one_epoch(model, train_loader, optimizer, loss_fn)\n",
        "        logger.info(f\"Epoch {ep} | Train Loss (MSE) {loss}\")\n",
        "        ## can add mlflow metrics logging / early stopping here\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model:torch_geometric.nn, loader:DataLoader, loss_fn:torch.nn) -> float:\n",
        "    \"\"\"\n",
        "    Compute validation loss for model\n",
        "    Args:\n",
        "      model: torch_geometric.nn model to be validated\n",
        "      loader: DataLoader for validation data å\n",
        "      loss_fn:l oss function from torch.nn\n",
        "\n",
        "    Returns:\n",
        "      valid_loss: validation loss (float)\n",
        "    \"\"\"\n",
        "    valid_loss = 0\n",
        "    num_data = 0\n",
        "    model.eval()\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        pred = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        loss = loss_fn(pred, data.y)\n",
        "        valid_loss += float(loss) * data.num_graphs ##expand loss on batch by batch size\n",
        "        num_data += data.num_graphs\n",
        "    valid_loss = valid_loss/num_data\n",
        "    return valid_loss\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(model:torch_geometric.nn, loader:DataLoader, loss_fn:torch.nn) -> float:\n",
        "    \"\"\"\n",
        "    Test/Evaluate the model\n",
        "    Args:\n",
        "      model: torch_geometric.nn model to be validated\n",
        "      loader: DataLoader for validation data\n",
        "      loss_fn: loss function from torch.nn\n",
        "\n",
        "    Returns:\n",
        "      mse: returns mse (float)\n",
        "    \"\"\"\n",
        "    output = []\n",
        "    model.eval()\n",
        "    for data in loader:\n",
        "        data = data.to(device)\n",
        "        pred = model(data.x, data.edge_index, data.edge_attr, data.batch)\n",
        "        loss = loss_fn(pred, data.y)\n",
        "        concatenated_batch = torch.cat((pred, data.y), dim=1) ## dim = [num_test, 2]\n",
        "        output.append(concatenated_batch)\n",
        "\n",
        "    # Stack the tensors along batch of data dimension\n",
        "    stacked_output = torch.cat(output, dim=0) #concat by rows - all data in batch\n",
        "\n",
        "    ## can add additional evaluation metrics and/or return actual output for plotting\n",
        "    mse = mean_squared_error(stacked_output[1], stacked_output[0])\n",
        "    logger.info(f\"Test Evaluation MSE: {mse}\")\n",
        "\n",
        "    return mse\n"
      ],
      "metadata": {
        "id": "Fq4AzchSn_bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inner CV for h-param optimization"
      ],
      "metadata": {
        "id": "sGeR-yLX_7X_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cv_objective(trial:int, k_inner:int, model:torch_geometric.nn, train_val_data:Dataset, loss_fn:torch.nn, num_epochs:int):\n",
        "    \"\"\"\n",
        "    Inner CV to perform h-param optimization.\n",
        "    Args:\n",
        "      trial: trial number\n",
        "      k_inner: number of inner CV folds\n",
        "      model: torch_geometric.nn model to be trained\n",
        "      train_val_data: Dataset containing train and validation data for inner CV\n",
        "      loss_fn: loss function from torch.nn\n",
        "\n",
        "    Returns: mean of inner cv loss\n",
        "\n",
        "    \"\"\"\n",
        "    logger.info(f'Performing inner CV hparam optimization')\n",
        "    fold = KFold(n_splits=k_inner, shuffle=True, random_state=0)\n",
        "    inner_cv_loss = []\n",
        "    # get hparams for this trial\n",
        "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=False)\n",
        "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=False)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr,\n",
        "                                weight_decay=weight_decay)\n",
        "    for fold_idx, (train_idx, valid_idx) in enumerate(fold.split(range(len(train_val_data)))):\n",
        "        train_data = torch.utils.data.Subset(dataset, train_idx)\n",
        "        valid_data = torch.utils.data.Subset(dataset, valid_idx)\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_data,\n",
        "            batch_size=32,\n",
        "            shuffle=True\n",
        "        )\n",
        "        valid_loader = DataLoader(\n",
        "            valid_data,\n",
        "            batch_size=32,\n",
        "            shuffle=True,\n",
        "        )\n",
        "        ## reset model\n",
        "        model.reset_parameters()\n",
        "        # train model\n",
        "        train(num_epochs, model, train_loader, optimizer, loss_fn)\n",
        "        # get valid loss\n",
        "        loss_v = validate(model, valid_loader, loss_fn)\n",
        "        inner_cv_loss.append(loss_v)\n",
        "    return np.mean(inner_cv_loss)\n"
      ],
      "metadata": {
        "id": "KsNC8AI2kelU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main Outer CV"
      ],
      "metadata": {
        "id": "8tZZFLSbqkK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nested_cv_experiment(config:dict):\n",
        "    \"\"\"\n",
        "    Run the nested CV experiment using config dictionary\n",
        "    Args:\n",
        "      config: config for the experiment, including model, dataset, loss, optimizer, k-folds settings and epochs.\n",
        "    \"\"\"\n",
        "    ## training experiment settings unpack\n",
        "    model = config['model']\n",
        "    dataset = config['dataset']\n",
        "    loss_fn = config['loss_fn']\n",
        "    optimizer_class = config['optimizer_class']\n",
        "    num_epochs = config['num_epochs']\n",
        "    batch_size = config['batch_size']\n",
        "    k_inner = config['k_inner']\n",
        "    k_outer = config['k_outer']\n",
        "    objective = config['objective']\n",
        "    n_trials = config['n_trials']\n",
        "    timeout_optuna = config['timeout_optuna']\n",
        "\n",
        "    cv_outer = KFold(n_splits=k_outer, shuffle=True, random_state=22)\n",
        "    eval_loss = []\n",
        "    ## Outer CV\n",
        "    for fold, (train_ids, test_ids) in enumerate(cv_outer.split(range(len(dataset)))):\n",
        "        logger.info(f\"{fold}-FOLD out of {k_outer}\")\n",
        "        logger.info(f\"Train length: {len(train_ids)} Validate length: {len(test_ids)} SUM:, {len(test_ids)+len(train_ids)}\")\n",
        "        train_data = torch.utils.data.Subset(dataset, train_ids)\n",
        "        test_data = torch.utils.data.Subset(dataset, test_ids)\n",
        "\n",
        "        ## start inner CV for finding the best hparams on this fold\n",
        "        model.to(device).reset_parameters()\n",
        "\n",
        "        logger.info('Optimizing hparams')\n",
        "        study = optuna.create_study(direction=\"minimize\")\n",
        "        cv_objective = partial(objective, k_inner=k_inner, model=model, train_val_data=train_data, loss_fn=loss_fn, num_epochs=num_epochs) # perform inner CV for h-param tuning using train data from outer CV split\n",
        "        study.optimize(cv_objective, n_trials=2, timeout=timeout_optuna)\n",
        "\n",
        "        best_params = study.best_trial.params\n",
        "        logger.info(\"Best h-params: \", best_params)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=best_params['lr'],\n",
        "                                    weight_decay=best_params['weight_decay'])\n",
        "\n",
        "        ## retrain model on best params optimizer\n",
        "        logger.info('Test on unseen data using the model+best hparams from inner loop')\n",
        "        model.reset_parameters()\n",
        "        train_loader = DataLoader(train_data, batch_size=32)\n",
        "        train(num_epochs, model, train_loader, optimizer, loss_fn)\n",
        "\n",
        "        test_loader = DataLoader(test_data, batch_size=32)\n",
        "        mse_fold = test(model, test_loader, loss_fn)\n",
        "\n",
        "        eval_loss.append(mse_fold)\n",
        "\n",
        "    logger.info(f'Mean MSE across Outer CV: {np.mean(eval_loss)}')"
      ],
      "metadata": {
        "id": "PPaMY8eyd7l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run experiment"
      ],
      "metadata": {
        "id": "wV-z4h_b__US"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Set path to data and working directory for dataset creation.\n",
        "root_path = '/content/drive/MyDrive/Colab Notebooks/data_gnn/data/'\n",
        "working_path = '/content/drive/MyDrive/Colab Notebooks/data_gnn/working'\n",
        "data_files = []\n",
        "for filename in os.listdir(root_path):\n",
        "    data_files.append(root_path+filename)\n",
        "\n",
        "print(data_files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmrJd23xbEiT",
        "outputId": "c26a87cf-8c0f-4ab4-e9e2-f79f5e406531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/Colab Notebooks/data_gnn/data/Lipophilicity.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking dataset  "
      ],
      "metadata": {
        "id": "lhzIELiQAGoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MoleculeDataset(root=working_path, filenames=data_files, smiles_label='smiles', target_label='exp')\n",
        "\n",
        "dataset[0], dataset[2]\n",
        "\n",
        "## Input data feature: in_dim=9, edge_dim=3"
      ],
      "metadata": {
        "id": "QLytgkWIaZX6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "167a105b-a3ad-4416-cbbb-8019f5ec6bf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Data(x=[22, 9], edge_index=[2, 48], edge_attr=[48, 3], smiles='CC(C)c1ccc2Oc3nc(N)c(cc3C(=O)c2c1)C(=O)O', y=[1, 1]),\n",
              " Data(x=[28, 9], edge_index=[2, 60], edge_attr=[60, 3], smiles='CN(C)C(=O)N[C@@H]1CC[C@@H](CCN2CCN(CC2)c3ccc(Cl)c(Cl)c3)CC1', y=[1, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "# set experiment config\n",
        "config = {\n",
        "    'dataset': MoleculeDataset(root=working_path, filenames=data_files, smiles_label='smiles', target_label='exp'),\n",
        "    'model': AttentiveFP(in_channels=9, hidden_channels=64, out_channels=1,\n",
        "                    edge_dim=3, num_layers=2, num_timesteps=2,\n",
        "                    dropout=0.2),\n",
        "    'loss_fn': MSELoss(),\n",
        "    'optimizer_class': torch.optim.Adam,\n",
        "    'scheduler_params': {'gamma': 0.9},\n",
        "    'num_epochs': 2,\n",
        "    'batch_size': 32,\n",
        "    'k_inner': 3,\n",
        "    'k_outer': 5,\n",
        "    'objective': cv_objective,\n",
        "    'n_trials': 2,\n",
        "    'timeout_optuna': 300\n",
        "}\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# get logger\n",
        "log_file_path = '/content/drive/MyDrive/Colab Notebooks/data_gnn/experiment_attentiveFP_nestedcv.log'\n",
        "logging.basicConfig(filename=log_file_path,\n",
        "                    format='%(asctime)s %(message)s',\n",
        "                    level=logging.DEBUG,\n",
        "                    filemode='w',  force=True)\n",
        "logger = logging.getLogger('log')\n",
        "\n",
        "# run experiment on current config\n",
        "nested_cv_experiment(config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJVQv54d_VfH",
        "outputId": "30f45b62-62fd-436b-e904-eacbaaf38918"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2024-07-24 05:21:41,071] A new study created in memory with name: no-name-fd7b5df7-cbc7-4386-beb4-5740d2018221\n",
            "[I 2024-07-24 05:22:09,241] Trial 0 finished with value: 1.5318815537861417 and parameters: {'lr': 0.003334173984119139, 'weight_decay': 0.00028616464182883617}. Best is trial 0 with value: 1.5318815537861417.\n",
            "[I 2024-07-24 05:22:27,674] Trial 1 finished with value: 1.538210201831091 and parameters: {'lr': 0.005635351299963177, 'weight_decay': 0.00015293744563843885}. Best is trial 0 with value: 1.5318815537861417.\n",
            "[I 2024-07-24 05:22:36,216] A new study created in memory with name: no-name-03668931-1e55-484d-82eb-66951b8b21e3\n",
            "[I 2024-07-24 05:22:54,550] Trial 0 finished with value: 1.3783218463261921 and parameters: {'lr': 0.0016545709857408645, 'weight_decay': 0.00027517323451462574}. Best is trial 0 with value: 1.3783218463261921.\n",
            "[I 2024-07-24 05:23:18,977] Trial 1 finished with value: 1.419530709584554 and parameters: {'lr': 0.003724471494165675, 'weight_decay': 0.00012503340162321419}. Best is trial 0 with value: 1.3783218463261921.\n",
            "[I 2024-07-24 05:23:27,646] A new study created in memory with name: no-name-394b20a2-1648-45de-9213-7b4a6fb0bb55\n",
            "[I 2024-07-24 05:23:46,188] Trial 0 finished with value: 1.5580342440378097 and parameters: {'lr': 0.0017858968002927604, 'weight_decay': 5.735102310829259e-05}. Best is trial 0 with value: 1.5580342440378097.\n",
            "[I 2024-07-24 05:24:04,815] Trial 1 finished with value: 1.452963460059393 and parameters: {'lr': 0.007071915864093, 'weight_decay': 9.409609076301746e-05}. Best is trial 1 with value: 1.452963460059393.\n",
            "[I 2024-07-24 05:24:13,539] A new study created in memory with name: no-name-6e297fdf-4e6f-448c-a4d4-b4d59cb8bfdb\n",
            "[I 2024-07-24 05:24:31,925] Trial 0 finished with value: 1.455774747190021 and parameters: {'lr': 0.004363113406275021, 'weight_decay': 0.00077887481420339}. Best is trial 0 with value: 1.455774747190021.\n",
            "[I 2024-07-24 05:24:51,559] Trial 1 finished with value: 1.410374124844869 and parameters: {'lr': 0.0005748992068976366, 'weight_decay': 0.00020887162007475683}. Best is trial 1 with value: 1.410374124844869.\n",
            "[I 2024-07-24 05:24:59,883] A new study created in memory with name: no-name-616f636e-52ad-4810-a1a6-28a1e24cbd03\n",
            "[I 2024-07-24 05:25:19,199] Trial 0 finished with value: 1.5480168688864937 and parameters: {'lr': 0.002594134592024522, 'weight_decay': 0.0009739466446802066}. Best is trial 0 with value: 1.5480168688864937.\n",
            "[I 2024-07-24 05:25:37,953] Trial 1 finished with value: 1.4567810694376628 and parameters: {'lr': 0.0062455781422037305, 'weight_decay': 2.14251205119343e-05}. Best is trial 1 with value: 1.4567810694376628.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check experiment log\n",
        "print(Path(log_file_path).read_text())"
      ],
      "metadata": {
        "id": "NI7y8ItYg1XD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "821d0538-6e32-4938-b93a-bfc39c6ad96b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-07-24 05:21:41,068 0-FOLD out of 5\n",
            "2024-07-24 05:21:41,068 Train length: 672 Validate length: 168 SUM:, 840\n",
            "2024-07-24 05:21:41,070 Optimizing hparams\n",
            "2024-07-24 05:21:41,072 Performing inner CV hparam optimization\n",
            "2024-07-24 05:21:46,893 Epoch 0 | Train Loss (MSE) 15.936441779136658\n",
            "2024-07-24 05:21:50,964 Epoch 1 | Train Loss (MSE) 1.6647600020681108\n",
            "2024-07-24 05:21:56,272 Epoch 0 | Train Loss (MSE) 3.217013052531651\n",
            "2024-07-24 05:22:02,080 Epoch 1 | Train Loss (MSE) 2.8320268988609314\n",
            "2024-07-24 05:22:05,707 Epoch 0 | Train Loss (MSE) 5.033824299063001\n",
            "2024-07-24 05:22:08,174 Epoch 1 | Train Loss (MSE) 2.2837983880724226\n",
            "2024-07-24 05:22:09,247 Performing inner CV hparam optimization\n",
            "2024-07-24 05:22:11,686 Epoch 0 | Train Loss (MSE) 8.617808018411909\n",
            "2024-07-24 05:22:14,362 Epoch 1 | Train Loss (MSE) 1.6483559438160487\n",
            "2024-07-24 05:22:18,131 Epoch 0 | Train Loss (MSE) 15.159045645168849\n",
            "2024-07-24 05:22:20,565 Epoch 1 | Train Loss (MSE) 1.8033708759716578\n",
            "2024-07-24 05:22:24,051 Epoch 0 | Train Loss (MSE) 5.042614281177521\n",
            "2024-07-24 05:22:26,480 Epoch 1 | Train Loss (MSE) 1.6506760290690832\n",
            "2024-07-24 05:22:27,681 Best h-params: \n",
            "2024-07-24 05:22:27,682 Test on unseen data using the model+best hparams from inner loop\n",
            "2024-07-24 05:22:31,838 Epoch 0 | Train Loss (MSE) 3.914900467509315\n",
            "2024-07-24 05:22:35,392 Epoch 1 | Train Loss (MSE) 1.5624845652353196\n",
            "2024-07-24 05:22:36,213 Test Evaluation MSE: 0.722318708896637\n",
            "2024-07-24 05:22:36,213 1-FOLD out of 5\n",
            "2024-07-24 05:22:36,214 Train length: 672 Validate length: 168 SUM:, 840\n",
            "2024-07-24 05:22:36,216 Optimizing hparams\n",
            "2024-07-24 05:22:36,218 Performing inner CV hparam optimization\n",
            "2024-07-24 05:22:38,644 Epoch 0 | Train Loss (MSE) 5.342071984495435\n",
            "2024-07-24 05:22:41,083 Epoch 1 | Train Loss (MSE) 1.543670458453042\n",
            "2024-07-24 05:22:45,128 Epoch 0 | Train Loss (MSE) 2.322832763195038\n",
            "2024-07-24 05:22:47,570 Epoch 1 | Train Loss (MSE) 1.6924865927015031\n",
            "2024-07-24 05:22:51,019 Epoch 0 | Train Loss (MSE) 4.43597686290741\n",
            "2024-07-24 05:22:53,460 Epoch 1 | Train Loss (MSE) 1.6088369914463587\n",
            "2024-07-24 05:22:54,552 Performing inner CV hparam optimization\n",
            "2024-07-24 05:22:58,619 Epoch 0 | Train Loss (MSE) 4.949854910373688\n",
            "2024-07-24 05:23:03,101 Epoch 1 | Train Loss (MSE) 1.3815630418913705\n",
            "2024-07-24 05:23:07,262 Epoch 0 | Train Loss (MSE) 6.172271268708365\n",
            "2024-07-24 05:23:10,793 Epoch 1 | Train Loss (MSE) 1.9695579409599304\n",
            "2024-07-24 05:23:14,792 Epoch 0 | Train Loss (MSE) 13.339376211166382\n",
            "2024-07-24 05:23:17,396 Epoch 1 | Train Loss (MSE) 2.808229446411133\n",
            "2024-07-24 05:23:18,978 Best h-params: \n",
            "2024-07-24 05:23:18,981 Test on unseen data using the model+best hparams from inner loop\n",
            "2024-07-24 05:23:22,577 Epoch 0 | Train Loss (MSE) 6.109011457079933\n",
            "2024-07-24 05:23:26,663 Epoch 1 | Train Loss (MSE) 1.7226936561720712\n",
            "2024-07-24 05:23:27,641 Test Evaluation MSE: 0.2620140314102173\n",
            "2024-07-24 05:23:27,641 2-FOLD out of 5\n",
            "2024-07-24 05:23:27,642 Train length: 672 Validate length: 168 SUM:, 840\n",
            "2024-07-24 05:23:27,645 Optimizing hparams\n",
            "2024-07-24 05:23:27,650 Performing inner CV hparam optimization\n",
            "2024-07-24 05:23:30,145 Epoch 0 | Train Loss (MSE) 5.288554285253797\n",
            "2024-07-24 05:23:32,609 Epoch 1 | Train Loss (MSE) 1.8185450519834245\n",
            "2024-07-24 05:23:36,108 Epoch 0 | Train Loss (MSE) 3.5731509072440013\n",
            "2024-07-24 05:23:38,643 Epoch 1 | Train Loss (MSE) 1.7386288855757033\n",
            "2024-07-24 05:23:42,664 Epoch 0 | Train Loss (MSE) 5.175659775733948\n",
            "2024-07-24 05:23:45,106 Epoch 1 | Train Loss (MSE) 1.770738192967006\n",
            "2024-07-24 05:23:46,191 Performing inner CV hparam optimization\n",
            "2024-07-24 05:23:48,636 Epoch 0 | Train Loss (MSE) 39.98580316134861\n",
            "2024-07-24 05:23:51,100 Epoch 1 | Train Loss (MSE) 3.5740478379385814\n",
            "2024-07-24 05:23:55,125 Epoch 0 | Train Loss (MSE) 5.271507126944406\n",
            "2024-07-24 05:23:57,724 Epoch 1 | Train Loss (MSE) 1.6066122182777949\n",
            "2024-07-24 05:24:01,270 Epoch 0 | Train Loss (MSE) 11.069071420601436\n",
            "2024-07-24 05:24:03,739 Epoch 1 | Train Loss (MSE) 2.022903621196747\n",
            "2024-07-24 05:24:04,817 Best h-params: \n",
            "2024-07-24 05:24:04,820 Test on unseen data using the model+best hparams from inner loop\n",
            "2024-07-24 05:24:08,987 Epoch 0 | Train Loss (MSE) 13.619297510101681\n",
            "2024-07-24 05:24:12,734 Epoch 1 | Train Loss (MSE) 1.81623075121925\n",
            "2024-07-24 05:24:13,534 Test Evaluation MSE: 0.10944317281246185\n",
            "2024-07-24 05:24:13,535 3-FOLD out of 5\n",
            "2024-07-24 05:24:13,535 Train length: 672 Validate length: 168 SUM:, 840\n",
            "2024-07-24 05:24:13,538 Optimizing hparams\n",
            "2024-07-24 05:24:13,541 Performing inner CV hparam optimization\n",
            "2024-07-24 05:24:15,979 Epoch 0 | Train Loss (MSE) 6.141749714102064\n",
            "2024-07-24 05:24:18,346 Epoch 1 | Train Loss (MSE) 1.726645222731999\n",
            "2024-07-24 05:24:22,254 Epoch 0 | Train Loss (MSE) 6.275875687599182\n",
            "2024-07-24 05:24:24,873 Epoch 1 | Train Loss (MSE) 1.7481226495334081\n",
            "2024-07-24 05:24:28,391 Epoch 0 | Train Loss (MSE) 5.5355300051825385\n",
            "2024-07-24 05:24:30,831 Epoch 1 | Train Loss (MSE) 1.5544853040150233\n",
            "2024-07-24 05:24:31,926 Performing inner CV hparam optimization\n",
            "2024-07-24 05:24:34,656 Epoch 0 | Train Loss (MSE) 3.097745827266148\n",
            "2024-07-24 05:24:37,477 Epoch 1 | Train Loss (MSE) 1.8607906528881617\n",
            "2024-07-24 05:24:41,150 Epoch 0 | Train Loss (MSE) 3.533802500792912\n",
            "2024-07-24 05:24:43,680 Epoch 1 | Train Loss (MSE) 1.9327510637896401\n",
            "2024-07-24 05:24:47,507 Epoch 0 | Train Loss (MSE) 2.8283877543040683\n",
            "2024-07-24 05:24:50,385 Epoch 1 | Train Loss (MSE) 1.9458618674959456\n",
            "2024-07-24 05:24:51,561 Best h-params: \n",
            "2024-07-24 05:24:51,566 Test on unseen data using the model+best hparams from inner loop\n",
            "2024-07-24 05:24:55,335 Epoch 0 | Train Loss (MSE) 3.876853659039452\n",
            "2024-07-24 05:24:59,048 Epoch 1 | Train Loss (MSE) 1.7733380113329207\n",
            "2024-07-24 05:24:59,879 Test Evaluation MSE: 5.52259516553022e-05\n",
            "2024-07-24 05:24:59,880 4-FOLD out of 5\n",
            "2024-07-24 05:24:59,880 Train length: 672 Validate length: 168 SUM:, 840\n",
            "2024-07-24 05:24:59,882 Optimizing hparams\n",
            "2024-07-24 05:24:59,884 Performing inner CV hparam optimization\n",
            "2024-07-24 05:25:02,712 Epoch 0 | Train Loss (MSE) 10.467503564698356\n",
            "2024-07-24 05:25:05,483 Epoch 1 | Train Loss (MSE) 1.9678965977260046\n",
            "2024-07-24 05:25:09,104 Epoch 0 | Train Loss (MSE) 16.579386438642228\n",
            "2024-07-24 05:25:11,550 Epoch 1 | Train Loss (MSE) 2.285440240587507\n",
            "2024-07-24 05:25:15,162 Epoch 0 | Train Loss (MSE) 2.4732250911848888\n",
            "2024-07-24 05:25:18,096 Epoch 1 | Train Loss (MSE) 1.801707148551941\n",
            "2024-07-24 05:25:19,201 Performing inner CV hparam optimization\n",
            "2024-07-24 05:25:21,618 Epoch 0 | Train Loss (MSE) 23.205281921795436\n",
            "2024-07-24 05:25:24,100 Epoch 1 | Train Loss (MSE) 2.091872070516859\n",
            "2024-07-24 05:25:27,660 Epoch 0 | Train Loss (MSE) 8.501939611775535\n",
            "2024-07-24 05:25:30,497 Epoch 1 | Train Loss (MSE) 1.8537846037319727\n",
            "2024-07-24 05:25:34,347 Epoch 0 | Train Loss (MSE) 4.72958231823785\n",
            "2024-07-24 05:25:36,834 Epoch 1 | Train Loss (MSE) 1.655681073665619\n",
            "2024-07-24 05:25:37,955 Best h-params: \n",
            "2024-07-24 05:25:37,957 Test on unseen data using the model+best hparams from inner loop\n",
            "2024-07-24 05:25:41,701 Epoch 0 | Train Loss (MSE) 7.879542714073544\n",
            "2024-07-24 05:25:45,919 Epoch 1 | Train Loss (MSE) 1.8274941444396973\n",
            "2024-07-24 05:25:46,759 Test Evaluation MSE: 1.2739683389663696\n",
            "2024-07-24 05:25:46,760 Mean MSE across Outer CV: 0.4735598564147949\n",
            "\n"
          ]
        }
      ]
    }
  ]
}